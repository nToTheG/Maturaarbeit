\chapter{\chapThree}
\label{cha:chapter3} % Label for hyperlink

% Start font-size
\begingroup
\fontsize{12pt}{14pt}\selectfont

In diesem Kapitel wird beschrieben, wie die im Kapitel 2 vorgestellten theoretischen Grundlagen praktisch umgesetzt werden.
Ziel ist es, eine Gestensteuerung der Crazyflie 2.0 mithilfe von ArUco-Markern zu realisieren.
Um dieses Ziel zu erreichen, wurde eine Methode entwickelt, die sowohl die Erkennung und Analyse von Handbewegungen als auch die Steuerung der Drohne umfasst.

\section{Vorgehensweise}

Diese Arbeit umfasst mehrere Prozessschritte, die von der Erfassung visueller Daten über deren mathematische Verarbeitung bis hin zur Umsetzung in Steuerbefehle für die Drohne reichen.
Dazu werden folgende Schritte durchgeführt:

\begin{enumerate}
    \item \textbf{Erfassen der Handbewegung:}
    \begin{itemize}
        \item Kameraerfassung der ArUco-Marker auf der Hand.
        \item Verarbeitung der Bilder mit OpenCV.
        \item Extraktion der Markerpositionen und Eckpunkte.
    \end{itemize}
    \item \textbf{Datenverarbeitung und Interpretation:}
    \begin{itemize}
        \item Flächenberechnung jedes Markers mittels Gaußscher Trapezformel.
        \item Bestimmung der Ausrichtung der Handfläche.
    \end{itemize}
    \item \textbf{Steuerung der Crazyflie:}
    \begin{itemize}
        \item Übersetzung der Handausrichtung in Steuerargumente.
        \item Verwendung dieser Argumente in den CFLib-Funktionen.
        \item Übertragung der Befehle über das Crazyradio PA via CRTP.
    \end{itemize}
\end{enumerate}

\section{Erfassen der Handbewegung}

Zu Beginn muss die OpenCV-Bibliothek in das Python-Skript importiert werden, da sie die grundlegenden Funktionen für die Bildverarbeitung bereitstellt.
Für die Erfassung der Handbewegung ist der Zugriff auf die Kamera erforderlich.
Dies erfolgt über die Klasse \bodyCode{VideoCapture()}, die eine Verbindung zur Kamera herstellt und kontinuierlichen Zugriff auf deren Bilddaten ermöglicht.
Ob die Kamera erfolgreich geöffnet wurde oder beispielsweise durch ein anderes Programm blockiert ist, kann mit der integrierten Funktion \bodyCode{isOpened()} überprüft werden.

Um sicherzustellen, dass tatsächlich Bildinformationen empfangen werden, muss in einer Schleife die \bodyCode{read()}-Funktion aufgerufen werden.
Diese gibt zwei Werte zurück:

\begin{itemize}
    \item Der erste Wert gibt an, ob ein Bild erfolgreich empfangen wurde\footnotemark{}.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{success} ersichtlich.}
    \item Der zweite Wert enthält das aktuelle Kamerabild in Form eines NumPy-Arrays\footnotemark{}.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{frame} ersichtlich.}
\end{itemize}

Wenn kein Bild empfangen werden konnte, wird das Programm beendet.
Anschliessend wird das Bild in Graustufen konvertiert, um die spätere Erkennung der ArUco-Marker zu erleichtern.
Die Spiegelung entlang der vertikalen Achse sorgt dafür, dass das angezeigte Bild der realen Ausrichtung der Hand entspricht.

Damit im aufgenommenen Bild nach ArUco-Markern gesucht werden kann, muss zunächst die passende ArUco-Bibliothek definiert werden.
Das geschieht einmalig mit dem Befehl

\bodyCode{cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT\_4X4\_50)}.

Wie bereits in \hTeLi{sub:fw}{Kapitel~\ref{sub:fw}} erwähnt, werden im Rahmen dieser Arbeit Marker aus der \bodyCode{4X4\_50}-Bibliothek verwendet.
Mit \bodyCode{DetectorParameters()} wird eine Instanz der im ArUco-Modul enthaltenen Parameterklasse erstellt.
Diese Klasse definiert unter anderem die internen Abläufe der Markererkennung wie beispielsweise die Kantenerkennung oder die Filterung von Fehlkandidaten.

Das Parameterobjekt und die festgelegte Bibliothek werden anschliessend als Argumente an die Funktion \bodyCode{ArucoDetector()} übergeben, welche den eigentlichen Erkennungsablauf initialisiert.
Die Detektion erfolgt durch den Aufruf von \bodyCode{detectMarkers()}, der das aktuelle Bild als Argument entgegennimmt.
Als Rückgabewert erhält man eine geordnete Sammlung (ein sogenanntes Tupel) dreier Elemente.

\begin{itemize}
    \item Das erste Element enthält die Eckkoordinaten der erkannten Marker in Form von NumPy-Arrays.
    \item Das zweite Element enthält die zugehörigen Marker-IDs.
    \item Das dritte Element umfasst Marker-Kandidaten\footnotemark{}, die zwar als potenzielle Marker erkannt, aber nach der Validierung verworfen wurden.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{rejected} ersichtlich.}
\end{itemize}

\codefile[detection.py]{Codeausschnitt: Initialisierung der Kamera und Erfassung der ArUco-Marker}{sni:det}

\subsection{Potenzielle Probleme}
Bei der Erkennung der ArUco-Marker wird davon ausgegangen, dass verschiedene Faktoren die Genauigkeit und Stabilität der Resultate beeinflussen könnten.

Ein wesentlicher Einflussfaktor dürften die Lichtverhältnisse sein.
Starke Reflexionen, Schatten oder ungleichmässige Beleuchtung könnten dazu führen, dass die Kanten der Marker nicht eindeutig erkannt werden.
Dies würde sich insbesondere auf die Kantenerkennung und damit auf die Validierung der Marker auswirken.

Auch die Grösse der Marker sowie der Abstand zur Kamera könnten eine Rolle spielen.
Befinden sich die Marker zu weit entfernt oder werden sie in zu kleiner Auflösung aufgenommen, wären die schwarzen und weissen Bereiche nicht mehr klar voneinander unterscheidbar, was die Erkennung erschweren könnte.
Ebenso wäre es denkbar, dass eine teilweise Verdeckung oder eine starke Neigung der Hand dazu führt, dass Marker nicht erkannt oder falsch identifiziert würden.

Darüber hinaus könnte Bewegung während der Aufnahme zu unscharfen Bildern führen.
Dies träfe insbesondere dann zu, wenn die Framerate der Kamera zu niedrig oder die Bewegung der Hand zu schnell wäre.

\section{Datenverarbeitung und Interpretation}

Als Nächstes soll die Fläche jedes erkannten Markers berechnet werden.
Dafür müssen die Eckkoordinaten zunächst in ein handlicheres Format überführt werden.
Das gewünschte Format ist eine Liste, die für jeden Marker eine weitere Liste enthält.\footnotemark{}
\footnotetext{Im \hTeLi{sni:slf}{Codeausschnitt} als \bodyCode{formatted\_markers} visualisiert.}
Innerhalb dieser Listen sind die Koordinaten der vier Eckpunkte jeweils als Tupel gespeichert.

Mit dieser Datenstruktur lässt sich anschliessend die Fläche jedes Markers mithilfe der Shoelace-Formel berechnen, wie in \hTeLi{sni:slf}{Codeausschnitt} dargestellt.

\codefile[shoelace.py]{Codeausschnitt: Algorithmus Shoelace-Formel}{sni:slf}

Zur Auswertung der Handbewegung wird auf das in \hTeLi{sub:quska}{Kapitel~\ref{sub:quska}} eingeführte Prinzip der quadratischen Skalierung und Verhältnisbildung zurückgegriffen.
Die Markerflächen werden also zunächst quadriert, in die vier Gruppen (oben, unten, rechts, links) eingeordnet und die Summen sowie Quotienten \inlinemath{Q_\text{OU}} und \inlinemath{Q_\text{RL}} gebildet.
Anhand dieser Quotienten lässt sich die dominierende Bewegungsrichtung der Hand bestimmen:

\begin{itemize}
    \item \inlinemath{Q_\text{OU} < Q_\text{RL}}: horizontale Bewegung (links/rechts) überwiegt
    \item \inlinemath{Q_\text{OU} > Q_\text{RL}}: vertikale Bewegung (vorwärts/rückwärts) überwiegt
\end{itemize}



\section{Steuerung der Crazyflie}
\label{sec:cf_co}

% End font-size
\endgroup
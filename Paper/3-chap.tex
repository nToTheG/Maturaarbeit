\chapter{\chapThree}
\label{cha:chapter3} % Label for hyperlink

% Start font-size
\begingroup
\fontsize{12pt}{14pt}\selectfont

In diesem Kapitel wird beschrieben, wie die in \hTeLi{cha:chapter2}{Kapitel~\ref{cha:chapter2}} vorgestellten theoretischen Grundlagen praktisch umgesetzt werden.
Ziel ist es, eine Gestensteuerung der Crazyflie 2.0 mithilfe von ArUco-Markern zu realisieren.
Um dieses Ziel zu erreichen, wurde eine zwei Methoden entwickelt, die sowohl die Erkennung und Analyse von Handbewegungen als auch die Steuerung der Drohne umfasst.

\section{Vorgehensweise}

Beide Methoden umfassen die selben Prozessschritte, die von der Erfassung visueller Daten über deren mathematische Verarbeitung bis hin zur Umsetzung in Steuerbefehle für die Drohne reichen.
Der ausschlaggebende Unterschied ist dabei die Platzierung der Marker auf der Hand (mehr dazu in \hTeLi{sec:diff_meth}{Kapitel~\ref{sec:diff_meth}}).
Innerhalb beider Methoden werden folgende Schritte durchgeführt:

\begin{enumerate}
    \item \textbf{Erfassen der Handbewegung:}
    \begin{itemize}
        \item Kameraerfassung der ArUco-Marker auf der Hand.
        \item Verarbeitung der Bilder mit OpenCV.
        \item Extraktion der Markerpositionen und Eckpunkte.
    \end{itemize}
    \item \textbf{Datenverarbeitung und Interpretation:}
    \begin{itemize}
        \item Flächenberechnung jedes Markers mittels Gausscher Trapezformel.
        \item Einmaliges Kalibrieren der Marker.
        \item Bestimmung der Ausrichtung der Handfläche.
    \end{itemize}
    \item \textbf{Steuerung der Crazyflie:}
    \begin{itemize}
        \item Übersetzung der Handausrichtung in Steuerargumente.
        \item Verwendung dieser Argumente in den CFLib-Funktionen.
        \item Übertragung der Befehle über das Crazyradio PA via CRTP.
    \end{itemize}
\end{enumerate}

\section{Unterschiede der Lösungsansätze}
\label{sec:diff_meth}

Wie bereits erwähnt, unterscheiden sich die Methoden in der Platzierung der ArUco-Marker auf der Hand.
Beide Methoden verwenden genau vier Marker, welche die IDs 1 bis 4 darstellen.

\subsection{Methode 1}
In Methode~1 werden die Marker in einem Rechteck platziert.
Als Referenzpunkte dienen dazu die Fingergrundgelenke (\textit{FFG}) und das Handgelenk (\textit{HG}) der Hand:

\begin{itemize}
    \item Marker \bodyCode{ID=1} wird auf das \textit{FGG} des kleinen Fingers geklebt.
    \item Marker \bodyCode{ID=2} wird auf das \textit{FGG} des Indexfingers geklebt.
    \item Marker \bodyCode{ID=3} wird auf die rechte Seite des \textit{HG}s geklebt.
    \item Marker \bodyCode{ID=4} wird auf die linke Seite des \textit{HG}s geklebt.
\end{itemize}

Die Breite des Markerbereichs ist hier als Distanz zwischen den Mittelpunkten der ArUco-Marker mit den IDs 1 und 2 definiert.
Die Höhe ist mit der Entfernung der Mittelpunkte zwischen Marker \bodyCode{ID=1} und \bodyCode{ID=4} gleichzusetzen.

\subsection{Methode 2}
Im Gegensatz zur Methode~1 werden bei Methode~2 die Finger als Teil der Hand mitverwendet.
Markern mit \bodyCode{ID=1} soll auf der Spitze des kleinen Fingers befestigt werden.
Gegenüberliegend davon ist auf der Daumenspitze der Marker mit der ID 3 anzubringen
Die übrigen Marker werden auf die Spitze des Mittelfingers (\bodyCode{ID=2}) und zentriert auf das HG (\bodyCode{ID=4}) geklebt.
Die Tags bilden somit ein Viereck in Form einer Raute.

Auch bei dieser Version muss die Breite und die Höhe des von Markern umschlossenen Bereichs bestimmt werden.

\begin{itemize}
    \item \textbf{Breite:} Mittelpunktentfernung der Markern des kleinen Fingers und des Daumens.
    \item \textbf{Höhe:} Distanz zwischen Mittelpunkt des \textit{HG}-Tags und des Mittelfingertags.
\end{itemize}

\section{Erfassen der Handbewegung}
Zu Beginn muss die OpenCV-Bibliothek in das Python-Skript importiert werden, da sie die grundlegenden Funktionen für die Bildverarbeitung bereitstellt.
Für die Erfassung der Handbewegung ist der Zugriff auf die Kamera erforderlich.
Dies erfolgt über die Klasse \bodyCode{VideoCapture()}, die eine Verbindung zur Kamera herstellt und kontinuierlichen Zugriff auf deren Bilddaten ermöglicht.
Ob die Kamera erfolgreich geöffnet wurde oder beispielsweise durch ein anderes Programm blockiert ist, kann mit der integrierten Funktion \bodyCode{isOpened()} überprüft werden.

Um sicherzustellen, dass tatsächlich Bildinformationen empfangen werden, muss in einer Schleife die \bodyCode{read()}-Funktion aufgerufen werden.
Diese gibt zwei Werte zurück:

\begin{itemize}
    \item Der erste Wert gibt an, ob ein Bild erfolgreich empfangen wurde\footnotemark{}.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{success} ersichtlich.}
    \item Der zweite Wert enthält das aktuelle Kamerabild in Form eines NumPy-Arrays\footnotemark{}.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{frame} ersichtlich.}
\end{itemize}

Wenn kein Bild empfangen werden konnte, wird das Programm beendet.
Anschliessend wird das Bild in Graustufen konvertiert, um die spätere Erkennung der ArUco-Marker zu erleichtern.

Damit im aufgenommenen Bild nach ArUco-Markern gesucht werden kann, muss zunächst die passende ArUco-Bibliothek definiert werden.
Das geschieht einmalig mit dem Befehl

\bodyCode{cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT\_4X4\_50)}.

Wie bereits in \hTeLi{sub:fw}{Kapitel~\ref{sub:fw}} erwähnt, werden im Rahmen dieser Arbeit Marker aus der \bodyCode{4X4\_50}-Bibliothek verwendet.
Mit \bodyCode{DetectorParameters()} wird eine Instanz der im ArUco-Modul enthaltenen Parameterklasse erstellt.
Diese Klasse definiert unter anderem die internen Abläufe der Markererkennung wie beispielsweise die Kantenerkennung oder die Filterung von Fehlkandidaten.

Das Parameterobjekt und die festgelegte Bibliothek werden anschliessend als Argumente an die Funktion \bodyCode{ArucoDetector()} übergeben, welche den eigentlichen Erkennungsablauf initialisiert.
Die Detektion erfolgt durch den Aufruf von \bodyCode{detectMarkers()}, der das aktuelle Bild als Argument entgegennimmt.
Als Rückgabewert erhält man eine geordnete Sammlung (ein sogenanntes Tupel) dreier Elemente.

\begin{itemize}
    \item Das erste Element enthält die Eckkoordinaten der erkannten Marker in Form von NumPy-Arrays.
    \item Das zweite Element enthält die zugehörigen Marker-IDs.
    \item Das dritte Element umfasst Marker-Kandidaten\footnotemark{}, die zwar als potenzielle Marker erkannt, aber nach der Validierung verworfen wurden.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{rejected} ersichtlich.}
\end{itemize}

\codefile[detection.py]{Codeausschnitt: Initialisierung der Kamera und Erfassung der ArUco-Marker}{sni:det}

Hierbei ist es wichtig zu beachten, dass die \inlinemath{y}-Achse eines von von OpenCV aufgenommenen Kamerabildes invertiert.
Das bedeutet, das Pixel mit den Koordinaten (0|0) ist im oberen linken Ecken des Bildes.

\subsection{Potenzielle Probleme}
Bei der Erkennung der ArUco-Marker wird davon ausgegangen, dass verschiedene Faktoren die Genauigkeit und Stabilität der Resultate beeinflussen könnten.

Ein wesentlicher Einflussfaktor dürften die Lichtverhältnisse sein.
Starke Reflexionen, Schatten oder ungleichmässige Beleuchtung könnten dazu führen, dass die Kanten der Marker nicht eindeutig erkannt werden.
Dies würde sich insbesondere auf die Kantenerkennung und damit auf die Validierung der Marker auswirken.

Auch die Grösse der Marker sowie der Abstand zur Kamera könnten eine Rolle spielen.
Befinden sich die Marker zu weit entfernt oder werden sie in zu kleiner Auflösung aufgenommen, wären die schwarzen und weissen Bereiche nicht mehr klar voneinander unterscheidbar, was die Erkennung erschweren könnte.
Ebenso wäre es denkbar, dass eine teilweise Verdeckung oder eine starke Neigung der Hand dazu führt, dass Marker nicht erkannt oder falsch identifiziert würden.

Darüber hinaus könnte Bewegung während der Aufnahme zu unscharfen Bildern führen.
Dies träfe insbesondere dann zu, wenn die Framerate der Kamera zu niedrig oder die Bewegung der Hand zu schnell wäre.

\section{Datenverarbeitung und Interpretation}
Innerhalb der Datenverarbeitung unterscheidet man zwischen zwei verschiedenen Funktionen.
Der erste beinhaltet die Kalibrierung der Referenzdaten.
ist diese einmal fertig, wird sie icht mehr verwendet.

\subsection{Kalibrieren der Marker}
Der Zweck der Kalibrierung ist es, Referenzdaten für die spätere Bestimmung der Ausrichtung zu haben.\footnotemark{}
\footnotetext{Diese Momentaufnahmen werden von nun an als \textit{Snapshots} bezeichnet}
Die gespeicherten Daten bestehen zum einen aus der Höhe und der Breite des erkannten Markerbereiches.
Dafür muss zunächst die Hand mittig auf dem Kamerabild platziert werden.
Ein Knopfdruck startet die Kalibrierung. 
Nun müssen zunächst die Mittelpunkte derjenigen Marker bestimmt, die für die Berechnung der Höhe und Breite benötigt sind.
Anschliessend kann mit Vektorsubtraktion und dem Satz des Pythagoras die Distanz zwischen den Referenzpunkte berechnet werden.

Zusätzlich müssen noch die Fläche der Marker gespeichert werden.
Die Shoelace-Formel ist hierfür der genau richtige Ansatz.

\subsection{Richtungsbestimmung}
In dieser Arbeit wird ein Ausschlussverfahren für die Richtungsbestimmung verwendet.
Es soll eingangs die Horizontale und Vertikale des erkannten Markerbereichs berechnet und anschliessend mit den gespeicherten \textit{Snapshot}s veglichen werden.
Dafür werden wiederum die Mittelpunkte ausschlaggebender Tags bestimmt.

Weil die Marker in \textbf{beiden} Versionen nicht quadratisch exakt angeordnet sind, müssen die Verhältnisse miteinander verglichen werden.
Ist der Quotient aus dem \textit{Snapshot} der Breite und der aktuellen Breite grösser als der der Höhe, so ist eine Drehung um die \inlinemath{x}-Achse ausgeschlossen.
In diesem Fall würde es sich also um eine Drehung and der \inlinemath{y}-Achse handeln.
Ob es sich um eine Drehung nach links oder rechts handelt, wird durch einen weiteren Vergleich mit den gespeicherten \textit{Snapshots} ermittelt.
Ist die Fläche der rechtsliegenden Marker kleiner als die korrespondierende Fläche im \textit{Snapshot} und die Fläche der linken Marker grösser als die der rechten, so ist eine Linksdrehung vorhanden.
Somit wurde die Anwinkelung der Hand interpretiert.
Sie beschreibt jedoch keine seitliche Verschiebung, sondern eine Drehung um die vertikale Achse (Yaw-Bewegung) oder eine Vorwärts- oder Rückwärtsbewegung.

Damit auch die vertikale Position der Hand im Bild erfasst werden kann, wird die \inlinemath{y}-Koordinate des Markerbereichs berechnet.
Der Mittelpunkt des Markerbereichs ist gleich dem Mittelpunkt des Vierecks, dass die Mittelpunkte der marker als Eckpunkte hat.
Anschliessend vergleicht man diesen Wert mit der Hälfte der Bildhöhe.
Liegt der Mittelpunkt deutlich oberhalb dieser Bildmitte, so wird die Hand als \enquote{oben} interpretiert; liegt er deutlich darunter, als \enquote{unten}.

Um zufällige oder geringe Schwankungen der Hand nicht mit einem Bewegungsbefehl zu verwechseln, werden sogenannte \textit{Deadzones} definiert.
Für Drehung um die \inlinemath{x}- oder \inlinemath{y}-Achse bedeutet das, dass der Quotient der einen Distanz und des dazugehörigen \textit{Snapshot}-Werts grösser als die Summe aus dem Quotient der anderen Distanzwerte und einem gewissen \textit{Deadzone}-Werts sein muss.

\section{Steuerung der Crazyflie}
\label{sec:cf_co}
Um eine Crazyflie zu steuern, müssen mehrere Module der \bodyCode{CFlib}-Bibliothek importiert werden:

\begin{itemize}
    \item Die \bodyCode{Crazyflie}-Klasse stellt die grundlegende Kommunikationsschnittstelle zur Drohne bereit.
    \item Die \bodyCode{SyncCrazyflie}-Klasse verwaltet die Verbindung zur Drohne und sorgt für einen synchronisierten Datenaustausch.
    \item Die \bodyCode{MotionCommander}-Klasse ermöglicht Streckenbefehle, wie in \hTeLi{sub:v2}{Kapitel~\ref{sub:v2}} beschrieben.
\end{itemize}

Die Verbindung zur Crazyflie wird mit einer \bodyCode{with}-Anweisung hergestellt.
Dabei wird eine Instanz der \bodyCode{SyncCrazyflie} mit einem entsprechenden URI als Argument erstellt.
Innerhalb dieses Kontexts kann eine \bodyCode{MotionCommander}-Instanz\footnotemark{} erzeugt werden, die eine definierte Flughöhe vorgibt.
Anschliessend können, basierend auf der aus den ArUco-Markern interpretierten Handbewegung, Steuerbefehle an die Crazyflie gesendet werden.
\footnotetext{Teilweise als \bodyCode{mc} ersichtlicht.}

Die mögliche Befehle sind in Tabelle~\TeLi{tab:cf_cmds} dargestellt.
Die Parameter \bodyCode{V\_YAW}, \bodyCode{V\_ALT} und \bodyCode{V\_HOR} definieren dabei die jeweiligen Geschwindigkeiten der Bewegungen:

\begin{itemize}
    \item \bodyCode{V\_YAW} beschreibt die Drehgeschwindigkeit um die vertikale Achse (Yaw), angegeben in m/s.
    \item \bodyCode{V\_ALT} bezeichnet die vertikale Steig- oder Sinkgeschwindigkeit in m/s.
    \item \bodyCode{V\_HOR} steht für die horizontale Fluggeschwindigkeit (Vorwärts oder Rückwärts) in m/s.
\end{itemize}

Die Werte dieser Konstanten werden so gewählt, dass die Bewegungen der Crazyflie sanft und kontrollierbar bleiben.
Zu hohe Geschwindigkeiten würden das präzise Steuern über Gesten erschweren.

\begin{table}[H]
    \centering
    \caption{Steuerbefehle des \bodyCode{MotionCommander}}
        \label{tab:cf_cmds}
    \begin{tabular}{l|l}
        \textbf{Richtung} & \textbf{MotionCommander-Befehl} \\ \hline
        Rechtsdrehung & \bodyCode{mc.start\_turn\_right(V\_YAW)} \\
        Linksdrehung & \bodyCode{mc.start\_turn\_left(V\_YAW)} \\
        Sinken & \bodyCode{mc.start\_down(V\_ALT)} \\
        Steigen & \bodyCode{mc.start\_up(V\_ALT)} \\
        Rückwärts & \bodyCode{mc.start\_back(V\_HOR)} \\
        Vorwärts & \bodyCode{mc.start\_forward(V\_HOR)} \\
    \end{tabular}
\end{table}

Die Klasse \bodyCode{MotionCommander} sendet kontinuierlich Bewegungsanweisungen an die Crazyflie.
Werden keine neuen Befehle übermittelt, beginnt sie automatisch mit der Landung.
Um dies zu verhindern, werden \textit{leere} Befehle an die Drohne gesendet, sofern keine Anweisungen aus der ArUco-Erkennung vorliegen.
Ein solcher leerer Befehl lautet:
\begin{center}
    \bodyCode{mc.start\_linear\_motion(0, 0, 0, rate\_yaw=0.0)}
\end{center}
Dieser Befehl teilt der Crazyflie mit, dass sie ihre aktuelle Position und Orientierung beibehalten soll.

\subsection{Sicherheitsmechanismen}
Als Sicherheitsmechanismus wurde eine Klasse implementiert, die eine Art Stopuhr-Funktion erfüllt.
Jedes Mal, wenn alle vier Marker auf der Hand erkannt werden, wird diese Stopuhr auf \inlinemath{t=0} zurückgesetzt.
Sie zählt kontinuierlich weiter und sobald sie \inlinemath{t=5} erreicht, wird die Crazyflie automatisch gelandet.
Damit ist der Landungsprozess abgesichert, und in einer Notsituation, beispielsweise bei einem Ausfall der Kamera, wird das Risiko einer Beschädigung der Drohne deutlich reduziert.

\codefile[stopwatch.py]{Codeausschnitt: Klasse einer Stopuhr-Funktion}{}

% End font-size
\endgroup
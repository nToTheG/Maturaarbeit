\chapter{\chapThree}
\label{cha:chapter3} % Label for hyperlink

% Start font-size
\begingroup
\fontsize{12pt}{14pt}\selectfont

In diesem Kapitel wird beschrieben, wie die im Kapitel 2 vorgestellten theoretischen Grundlagen praktisch umgesetzt werden.
Ziel ist es, eine Gestensteuerung der Crazyflie 2.0 mithilfe von ArUco-Markern zu realisieren.
Um dieses Ziel zu erreichen, wurde eine Methode entwickelt, die sowohl die Erkennung und Analyse von Handbewegungen als auch die Steuerung der Drohne umfasst.

\section{Vorgehensweise}

Diese Arbeit umfasst mehrere Prozessschritte, die von der Erfassung visueller Daten über deren mathematische Verarbeitung bis hin zur Umsetzung in Steuerbefehle für die Drohne reichen.
Dazu werden folgende Schritte durchgeführt:

\begin{enumerate}
    \item \textbf{Erfassen der Handbewegung:}
    \begin{itemize}
        \item Kameraerfassung der ArUco-Marker auf der Hand.
        \item Verarbeitung der Bilder mit OpenCV.
        \item Extraktion der Markerpositionen und Eckpunkte.
    \end{itemize}
    \item \textbf{Datenverarbeitung und Interpretation:}
    \begin{itemize}
        \item Flächenberechnung jedes Markers mittels Gaußscher Trapezformel.
        \item Bestimmung der Ausrichtung der Handfläche.
    \end{itemize}
    \item \textbf{Steuerung der Crazyflie:}
    \begin{itemize}
        \item Übersetzung der Handausrichtung in Steuerargumente.
        \item Verwendung dieser Argumente in den CFLib-Funktionen.
        \item Übertragung der Befehle über das Crazyradio PA via CRTP.
    \end{itemize}
\end{enumerate}

\section{Erfassen der Handbewegung}

Zu Beginn muss die OpenCV-Bibliothek in das Python-Skript importiert werden, da sie die grundlegenden Funktionen für die Bildverarbeitung bereitstellt.
Für die Erfassung der Handbewegung ist der Zugriff auf die Kamera erforderlich.
Dies erfolgt über die Klasse \bodyCode{VideoCapture()}, die eine Verbindung zur Kamera herstellt und kontinuierlichen Zugriff auf deren Bilddaten ermöglicht.
Ob die Kamera erfolgreich geöffnet wurde oder beispielsweise durch ein anderes Programm blockiert ist, kann mit der integrierten Funktion \bodyCode{isOpened()} überprüft werden.

Um sicherzustellen, dass tatsächlich Bildinformationen empfangen werden, muss in einer Schleife die \bodyCode{read()}-Funktion aufgerufen werden.
Diese gibt zwei Werte zurück:

\begin{itemize}
    \item Der erste Wert gibt an, ob ein Bild erfolgreich empfangen wurde\footnotemark{}.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{success} ersichtlich.}
    \item Der zweite Wert enthält das aktuelle Kamerabild in Form eines NumPy-Arrays\footnotemark{}.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{frame} ersichtlich.}
\end{itemize}

Wenn kein Bild empfangen werden konnte, wird das Programm beendet.
Anschliessend wird das Bild in Graustufen konvertiert, um die spätere Erkennung der ArUco-Marker zu erleichtern.

Damit im aufgenommenen Bild nach ArUco-Markern gesucht werden kann, muss zunächst die passende ArUco-Bibliothek definiert werden.
Das geschieht einmalig mit dem Befehl

\bodyCode{cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT\_4X4\_50)}.

Wie bereits in \hTeLi{sub:fw}{Kapitel~\ref{sub:fw}} erwähnt, werden im Rahmen dieser Arbeit Marker aus der \bodyCode{4X4\_50}-Bibliothek verwendet.
Mit \bodyCode{DetectorParameters()} wird eine Instanz der im ArUco-Modul enthaltenen Parameterklasse erstellt.
Diese Klasse definiert unter anderem die internen Abläufe der Markererkennung wie beispielsweise die Kantenerkennung oder die Filterung von Fehlkandidaten.

Das Parameterobjekt und die festgelegte Bibliothek werden anschliessend als Argumente an die Funktion \bodyCode{ArucoDetector()} übergeben, welche den eigentlichen Erkennungsablauf initialisiert.
Die Detektion erfolgt durch den Aufruf von \bodyCode{detectMarkers()}, der das aktuelle Bild als Argument entgegennimmt.
Als Rückgabewert erhält man eine geordnete Sammlung (ein sogenanntes Tupel) dreier Elemente.

\begin{itemize}
    \item Das erste Element enthält die Eckkoordinaten der erkannten Marker in Form von NumPy-Arrays.
    \item Das zweite Element enthält die zugehörigen Marker-IDs.
    \item Das dritte Element umfasst Marker-Kandidaten\footnotemark{}, die zwar als potenzielle Marker erkannt, aber nach der Validierung verworfen wurden.
    \footnotetext{Im \hTeLi{sni:det}{Codeausschnitt} als \bodyCode{rejected} ersichtlich.}
\end{itemize}

\codefile[detection.py]{Codeausschnitt: Initialisierung der Kamera und Erfassung der ArUco-Marker}{sni:det}

\subsection{Potenzielle Probleme}
Bei der Erkennung der ArUco-Marker wird davon ausgegangen, dass verschiedene Faktoren die Genauigkeit und Stabilität der Resultate beeinflussen könnten.

Ein wesentlicher Einflussfaktor dürften die Lichtverhältnisse sein.
Starke Reflexionen, Schatten oder ungleichmässige Beleuchtung könnten dazu führen, dass die Kanten der Marker nicht eindeutig erkannt werden.
Dies würde sich insbesondere auf die Kantenerkennung und damit auf die Validierung der Marker auswirken.

Auch die Grösse der Marker sowie der Abstand zur Kamera könnten eine Rolle spielen.
Befinden sich die Marker zu weit entfernt oder werden sie in zu kleiner Auflösung aufgenommen, wären die schwarzen und weissen Bereiche nicht mehr klar voneinander unterscheidbar, was die Erkennung erschweren könnte.
Ebenso wäre es denkbar, dass eine teilweise Verdeckung oder eine starke Neigung der Hand dazu führt, dass Marker nicht erkannt oder falsch identifiziert würden.

Darüber hinaus könnte Bewegung während der Aufnahme zu unscharfen Bildern führen.
Dies träfe insbesondere dann zu, wenn die Framerate der Kamera zu niedrig oder die Bewegung der Hand zu schnell wäre.

\section{Datenverarbeitung und Interpretation}

Als Nächstes soll die Fläche jedes erkannten Markers berechnet werden.
Dafür müssen die Eckkoordinaten zunächst in ein handlicheres Format überführt werden.
Das gewünschte Format ist eine Liste, die für jeden Marker eine weitere Liste enthält.\footnotemark{}
\footnotetext{Im \hTeLi{sni:slf}{Codeausschnitt} als \bodyCode{formatted\_markers} visualisiert.}
Innerhalb dieser Listen sind die Koordinaten der vier Eckpunkte jeweils als Tupel gespeichert.

Mit dieser Datenstruktur lässt sich anschliessend die Fläche jedes Markers mithilfe der Shoelace-Formel berechnen, wie im folgenden \hTeLi{sni:slf}{Skript} dargestellt.

\codefile[shoelace.py]{Codeausschnitt: Algorithmus Shoelace-Formel}{sni:slf}

Zur Auswertung der Handbewegung wird auf das in \hTeLi{sub:quska}{Kapitel~\ref{sub:quska}} eingeführte Prinzip der quadratischen Skalierung und Verhältnisbildung zurückgegriffen.
Die Markerflächen werden also zunächst quadriert, in die vier Gruppen (oben, unten, rechts, links) eingeordnet und die Summen sowie Quotienten \inlinemath{Q_\text{OU}} und \inlinemath{Q_\text{RL}} gebildet.
Anhand dieser Quotienten lässt sich die dominierende Bewegungsrichtung der Hand bestimmen:

\begin{itemize}
    \item \inlinemath{Q_\text{OU} < Q_\text{RL}}: horizontale Drehung (links/rechts) überwiegt
    \item \inlinemath{Q_\text{OU} > Q_\text{RL}}: vertikale Bewegung (vorwärts/rückwärts) überwiegt
\end{itemize}

Somit wurde die Anwinkelung der Hand interpretiert.
Sie beschreibt jedoch keine seitliche Verschiebung, sondern eine Drehung um die vertikale Achse (Yaw-Bewegung) oder eine Vorwärts- oder Rückwärtsbewegung.
Damit auch die vertikale Position der Hand im Bild erfasst werden kann, wird der Mittelpunkt\footnotemark{} der Handfläche auf der \inlinemath{y}-Achse berechnet.
\footnotetext{Wenn von Mittelpunkt gesprochen wird, ist damit der \inlinemath{y}-Wert des Mittelpunkts gemeint.}

Dazu werden zunächst die \inlinemath{y}-Mittelpunkte der beiden oberen und der beiden unteren Marker bestimmt.
Aus diesen Werten wird jeweils der Durchschnitt gebildet, wodurch der vertikale Mittelpunkt der oberen und der unteren Marker entsteht.
Der Mittelwert dieser beiden Punkte ergibt den \inlinemath{y}-Mittelpunkt der gesamten Handfläche.

Anschliessend vergleicht man diesen Wert mit der Hälfte der Bildhöhe.
Liegt der Mittelpunkt deutlich oberhalb dieser Bildmitte, so wird die Hand als \enquote{oben} interpretiert; liegt er deutlich darunter, als \enquote{unten}.
Um zufällige oder geringe Schwankungen zu vermeiden, wird eine sogenannte Deadzone definiert:
Erst wenn der Mittelpunkt mindestens um \SI{100}{Pixel} (etwa \SI{2.5}{\centi\meter}) über oder unter der Bildmitte liegt, wird eine vertikale Bewegung erkannt.

Ist \inlinemath{Q_\text{RL}} grösser, ist klar, dass die Drohne nach \inlinemath{\max{(S_\text{rechts}, S_\text{links})}} drehen muss.
Im Fall, dass \inlinemath{Q_\text{OU}} grösser ist, wird die Drohne nach \inlinemath{\max{(S_\text{oben}, S_\text{unten})}} fliegen.

\section{Steuerung der Crazyflie}
\label{sec:cf_co}
Um eine Crazyflie zu steuern, müssen mehrere Module der \bodyCode{CFlib}-Bibliothek importiert werden:

\begin{itemize}
    \item Die \bodyCode{Crazyflie}-Klasse stellt die grundlegende Kommunikationsschnittstelle zur Drohne bereit.
    \item Die \bodyCode{SyncCrazyflie}-Klasse verwaltet die Verbindung zur Drohne und sorgt für einen synchronisierten Datenaustausch.
    \item Die \bodyCode{MotionCommander}-Klasse ermöglicht Streckenbefehle, wie in \hTeLi{sub:v2}{Kapitel~\ref{sub:v2}} beschrieben.
\end{itemize}

Die Verbindung zur Crazyflie wird mit einer \bodyCode{with}-Anweisung hergestellt.
Dabei wird eine Instanz der \bodyCode{SyncCrazyflie} mit einem entsprechenden URI als Argument erstellt.
Innerhalb dieses Kontexts kann eine \bodyCode{MotionCommander}-Instanz\footnotemark{} erzeugt werden, die eine definierte Flughöhe vorgibt.
Anschliessend können, basierend auf der aus den ArUco-Markern interpretierten Handbewegung, Steuerbefehle an die Crazyflie gesendet werden.
\footnotetext{Teilweise als \bodyCode{mc} ersichtlicht.}

Die mögliche Befehle sind in Tabelle~\TeLi{tab:cf_cmds} dargestellt.
Die Parameter \bodyCode{V\_YAW}, \bodyCode{V\_ALT} und \bodyCode{V\_HOR} definieren dabei die jeweiligen Geschwindigkeiten der Bewegungen:

\begin{itemize}
    \item \bodyCode{V\_YAW} beschreibt die Drehgeschwindigkeit um die vertikale Achse (Yaw), angegeben in m/s.
    \item \bodyCode{V\_ALT} bezeichnet die vertikale Steig- oder Sinkgeschwindigkeit in m/s.
    \item \bodyCode{V\_HOR} steht für die horizontale Fluggeschwindigkeit (Vorwärts oder Rückwärts) in m/s.
\end{itemize}

Die Werte dieser Konstanten werden so gewählt, dass die Bewegungen der Crazyflie sanft und kontrollierbar bleiben.
Zu hohe Geschwindigkeiten würden das präzise Steuern über Gesten erschweren.

\begin{table}[H]
    \centering
    \caption{Steuerbefehle des \bodyCode{MotionCommander}}
        \label{tab:cf_cmds}
    \begin{tabular}{l|l}
        \textbf{Richtung} & \textbf{MotionCommander-Befehl} \\ \hline
        Rechtsdrehung & \bodyCode{mc.start\_turn\_right(V\_YAW)} \\
        Linksdrehung & \bodyCode{mc.start\_turn\_left(V\_YAW)} \\
        Sinken & \bodyCode{mc.start\_down(V\_ALT)} \\
        Steigen & \bodyCode{mc.start\_up(V\_ALT)} \\
        Rückwärts & \bodyCode{mc.start\_back(V\_HOR)} \\
        Vorwärts & \bodyCode{mc.start\_forward(V\_HOR)} \\
    \end{tabular}
\end{table}

Die Klasse \bodyCode{MotionCommander} sendet kontinuierlich Bewegungsanweisungen an die Crazyflie.
Werden keine neuen Befehle übermittelt, beginnt sie automatisch mit der Landung.
Um dies zu verhindern, werden \textit{leere} Befehle an die Drohne gesendet, sofern keine Anweisungen aus der ArUco-Erkennung vorliegen.
Ein solcher leerer Befehl lautet:
\begin{center}
    \bodyCode{mc.start\_linear\_motion(0, 0, 0, rate\_yaw=0.0)}
\end{center}
Dieser Befehl teilt der Crazyflie mit, dass sie ihre aktuelle Position und Orientierung beibehalten soll.

\subsection{Sicherheitsmechanismen}
Als Sicherheitsmechanismus wurde eine Klasse implementiert, die eine Art Stopuhr-Funktion erfüllt.
Jedes Mal, wenn alle vier Marker auf der Hand erkannt werden, wird diese Stopuhr auf \inlinemath{t=0} zurückgesetzt.
Sie zählt kontinuierlich weiter und sobald sie \inlinemath{t=5} erreicht, wird die Crazyflie automatisch gelandet.
Damit ist der Landungsprozess abgesichert, und in einer Notsituation, beispielsweise bei einem Ausfall der Kamera, wird das Risiko einer Beschädigung der Drohne deutlich reduziert.

% End font-size
\endgroup